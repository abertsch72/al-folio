---
---
@inproceedings{https://doi.org/10.48550/arxiv.2210.15462,
  doi = {10.48550/ARXIV.2210.15462},
	abbr={Findings},
  
  url = {https://arxiv.org/abs/2210.15462},
  
	show_url={true},
  author = {Bertsch, Amanda and Neubig, Graham and Gormley, Matthew R.},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {He Said, She Said: Style Transfer for Shifting the Perspective of Dialogues},
  
  publisher = {arXiv},
	show_url={true},
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022.",
  abstract = "In this work, we define a new style transfer task: perspective shift, which reframes a dialogue from informal first person to a formal third person rephrasing of the text. This task requires challenging coreference resolution, emotion attribution, and interpretation of informal text. We explore several baseline approaches and discuss further directions on this task when applied to short dialogues. As a sample application, we demonstrate that applying perspective shifting to a dialogue summarization dataset (SAMSum) substantially improves the zero-shot performance of extractive news summarization models on this data. Additionally, supervised extractive models perform better when trained on perspective shifted data than on the original dialogues. We release our code publicly.",
  year = {2022},
	selected={true},
  pdf = {https://arxiv.org/pdf/2210.15462.pdf},  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@inproceedings{bertsch-etal-2022-evaluating,
    title = "Evaluating Gender Bias Transfer from Film Data",
    author = "Bertsch, Amanda  and
      Oh, Ashley  and
      Natu, Sanika  and
      Gangu, Swetha  and
      Black, Alan W.  and
      Strubell, Emma",
    booktitle = "Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP)",
	abbr={GeBNLP},
    month = jul,
    year = "2022",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.gebnlp-1.24",
	selected={true},
    pages = "235--243",
	show_url={true},
    pdf={https://aclanthology.org/2022.gebnlp-1.24.pdf},
    abstract = "Films are a rich source of data for natural language processing. OpenSubtitles (Lison and Tiedemann, 2016) is a popular movie script dataset, used for training models for tasks such as machine translation and dialogue generation. However, movies often contain biases that reflect society at the time, and these biases may be introduced during pre-training and influence downstream models. We perform sentiment analysis on template infilling (Kurita et al., 2019) and the Sentence Embedding Association Test (May et al., 2019) to measure how BERT-based language models change after continued pre-training on OpenSubtitles. We consider gender bias as a primary motivating case for this analysis, while also measuring other social biases such as disability. We show that sentiment analysis on template infilling is not an effective measure of bias due to the rarity of disability and gender identifying tokens in the movie dialogue. We extend our analysis to a longitudinal study of bias in film dialogue over the last 110 years and find that continued pre-training on OpenSubtitles encodes additional bias into BERT. We show that BERT learns associations that reflect the biases and representation of each film era, suggesting that additional care must be taken when using historical data.",
}

@inproceedings{bertsch_detection_2021,
	address = {Online},
	abbr={W-NUT},
	title = {Detection of {Puffery} on the {English} {Wikipedia}},
	copyright = {All rights reserved},
	url = {https://aclanthology.org/2021.wnut-1.36},
	doi = {10.18653/v1/2021.wnut-1.36},
	abstract = {On Wikipedia, an online crowdsourced encyclopedia, volunteers enforce the encyclopedia's editorial policies. Wikipedia's policy on maintaining a neutral point of view has inspired recent research on bias detection, including “weasel words” and “hedges”. Yet to date, little work has been done on identifying “puffery,” phrases that are overly positive without a verifiable source. We demonstrate that collecting training data for this task requires some care, and construct a dataset by combining Wikipedia editorial annotations and information retrieval techniques. We compare several approaches to predicting puffery, and achieve 0.963 f1 score by incorporating citation features into a RoBERTa model. Finally, we demonstrate how to integrate our model with Wikipedia's public infrastructure to give back to the Wikipedia editor community.},
	urldate = {2022-02-06},
	booktitle = {Proceedings of the {Seventh} {Workshop} on {Noisy} {User}-generated {Text} ({W}-{NUT} 2021)},
	publisher = {Association for Computational Linguistics},
	author = {Bertsch, Amanda and Bethard, Steven},
	month = nov,
	show_url={true},
	selected={true},
	year = {2021},
	pages = {329--333},
	pdf = {https://aclanthology.org/2021.wnut-1.36.pdf},
	code_and_data = {https://github.com/abertsch72/wikipedia-puffery-detection},
}


